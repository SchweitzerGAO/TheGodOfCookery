{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下对150万数据的检索速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=\"./docs\",\n",
    "    recursive=True,\n",
    "    required_exts=[\".docx\"], # 只需要word文档\n",
    ")\n",
    "\n",
    "documents = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"/root/ZeroKaraNoRAG/models/bce-embedding-base_v1/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/hub/models--maidalun1020--bce-embedding-base_v1/snapshots/b0de52f5ffc2106da6b5a8b68577ed9052ea1a6f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import os\n",
    "HF_TOKEN=\"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\" # 你的huggingface token\n",
    "os.system(f\"huggingface-cli download --resume-download maidalun1020/bce-embedding-base_v1 --local-dir ./models/bce-embedding-base_v1 --local-dir-use-symlinks False --token {HF_TOKEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file models/bce-embedding-base_v1/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"models/bce-embedding-base_v1\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file models/bce-embedding-base_v1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at models/bce-embedding-base_v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"models/bce-embedding-base_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "探索嵌入模型的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/cook/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "SentenceTransformer                                               --\n",
       "├─Transformer: 1-1                                                --\n",
       "│    └─XLMRobertaModel: 2-1                                       --\n",
       "│    │    └─XLMRobertaEmbeddings: 3-1                             192,398,592\n",
       "│    │    └─XLMRobertaEncoder: 3-2                                85,054,464\n",
       "│    │    └─XLMRobertaPooler: 3-3                                 590,592\n",
       "├─Pooling: 1-2                                                    --\n",
       "├─Normalize: 1-3                                                  --\n",
       "==========================================================================================\n",
       "Total params: 278,043,648\n",
       "Trainable params: 278,043,648\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torchinfo import summary\n",
    "embedding_model_original=SentenceTransformer(\"models/bce-embedding-base_v1\")\n",
    "embedding_tokenizer=AutoTokenizer.from_pretrained(\"models/bce-embedding-base_v1\")\n",
    "summary(embedding_model_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试分词器是否能处理emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 6, 243544, 243691, 245561, 2], '<s> 🤗🤔👀</s>')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_tokenizer.encode(\"🤗🤔👀\"),embedding_tokenizer.decode([0, 6, 243544, 243691, 245561, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "探索嵌入模型对emoji的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def encode_sentence(sentence):\n",
    "    return torch.from_numpy(embedding_model_original.encode(sentence))\n",
    "\n",
    "emoji_list=[\"😭\",\"😢\",\"😱\",\"🤗\",\"😊\",\"🤩\",\"🏋🏼\",\"🚵🏻‍♂️\",\"🏊🏻‍♀️\"]\n",
    "emoji_embedding_list=[encode_sentence(emoji) for emoji in emoji_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这么一想甚至能从emoji中找出llm的看法、偏见等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list=[\"😊\",\"😡\",\"🐶\",\"🐱\"]\n",
    "emoji_embedding_list=[encode_sentence(emoji) for emoji in emoji_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list=[\"😊\",\"😡\",\"🇨🇳\",\"🇦🇲\",\"🇯🇵\"]\n",
    "emoji_embedding_list=[encode_sentence(emoji) for emoji in emoji_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list=[\"男人\",\"女人\"]\n",
    "emoji_embedding_list=[encode_sentence(emoji) for emoji in emoji_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😭与😢嵌入相似度0.48  😭与😱嵌入相似度0.75  😭与🤗嵌入相似度0.96  😭与😊嵌入相似度0.86  😭与🤩嵌入相似度0.96  😭与🏋🏼嵌入相似度0.96  😭与🚵🏻‍♂️嵌入相似度1.04  😭与🏊🏻‍♀️嵌入相似度0.98  \n",
      "😢与😱嵌入相似度0.76  😢与🤗嵌入相似度0.88  😢与😊嵌入相似度0.85  😢与🤩嵌入相似度0.95  😢与🏋🏼嵌入相似度0.93  😢与🚵🏻‍♂️嵌入相似度1.03  😢与🏊🏻‍♀️嵌入相似度0.99  \n",
      "😱与🤗嵌入相似度0.77  😱与😊嵌入相似度0.81  😱与🤩嵌入相似度0.72  😱与🏋🏼嵌入相似度0.81  😱与🚵🏻‍♂️嵌入相似度0.91  😱与🏊🏻‍♀️嵌入相似度0.90  \n",
      "🤗与😊嵌入相似度0.54  🤗与🤩嵌入相似度0.63  🤗与🏋🏼嵌入相似度0.82  🤗与🚵🏻‍♂️嵌入相似度0.92  🤗与🏊🏻‍♀️嵌入相似度0.95  \n",
      "😊与🤩嵌入相似度0.69  😊与🏋🏼嵌入相似度0.84  😊与🚵🏻‍♂️嵌入相似度0.92  😊与🏊🏻‍♀️嵌入相似度0.91  \n",
      "🤩与🏋🏼嵌入相似度0.71  🤩与🚵🏻‍♂️嵌入相似度0.86  🤩与🏊🏻‍♀️嵌入相似度0.91  \n",
      "🏋🏼与🚵🏻‍♂️嵌入相似度0.65  🏋🏼与🏊🏻‍♀️嵌入相似度0.77  \n",
      "🚵🏻‍♂️与🏊🏻‍♀️嵌入相似度0.39  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cos_similarity(x,y):\n",
    "    cosine_sim = torch.dot(x, y) / (torch.norm(x) * torch.norm(y))\n",
    "    return cosine_sim\n",
    "\n",
    "def L2(x,y):\n",
    "    return torch.norm(x-y,p=2)\n",
    "\n",
    "for i in range(len(emoji_list)):\n",
    "    for j in range(i+1, len(emoji_list)):\n",
    "        cos_sim = L2(emoji_embedding_list[i], emoji_embedding_list[j])  # 确保输入为浮\n",
    "        print(f\"{emoji_list[i]}与{emoji_list[j]}嵌入相似度{cos_sim:.2f}\",end=\"  \")\n",
    "    print()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "def de_punct(output: str):\n",
    "    rule = re.compile(r\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "    output = rule.sub('', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def f1_score(output, gt):\n",
    "    output = de_punct(output)\n",
    "    gt = de_punct(gt)\n",
    "    common = Counter(output) & Counter(gt)\n",
    "\n",
    "    # Same words\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    # precision\n",
    "    precision = 1.0 * num_same / len(output)\n",
    "\n",
    "    # recall\n",
    "    recall = 1.0 * num_same / len(gt)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1]) + 1\n",
    "\n",
    "    return dp[m][n]/max(len(s1),len(s2))\n",
    "\n",
    "def get_pair_distance(sentence_pair):\n",
    "    embedding1,embedding2=encode_sentence(sentence_pair)\n",
    "    return edit_distance(*sentence_pair),L2(embedding1,embedding2),f1_score(*sentence_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, tensor(0.8459), 1.0)\n",
      "(1.0, tensor(0.4579), 1.0)\n",
      "(0.42857142857142855, tensor(0.2250), 0.8571428571428571)\n",
      "(0.5, tensor(0.3034), 0.8235294117647058)\n",
      "(0.4166666666666667, tensor(0.4462), 0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "for i in [[\"蜜蜂\",\"蜂蜜\"],[\"牙刷\",\"刷牙\"],\n",
    "[\"大鲶鱼和小鲶鱼\",\"小鲶鱼与大鲶鱼\"],\n",
    "[\"大鲶鱼和小鲶鱼\",\"小鲶鱼与鲅鱼和大鲶鱼\"],\n",
    "[\"糖醋脆皮茄子的菜谱是什么\",\"糖醋脆皮茄子的做法\"]]:\n",
    "    print(get_pair_distance(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-index-vector-stores-faiss\n",
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "faiss_index=faiss.IndexFlatL2(768) # embedding的维度，这里用的bce\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义本地模型\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    # 需要严格对应模型的对话模板\n",
    "    return f\"<|im_start|>system\\n<|im_end|>\\n<|im_start|>user\\n{completion}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|im_start|>system\\n{message.content}<|im_end|>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|im_start|>user\\n{message.content}<|im_end|>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|im_start|>assistant\\n{message.content}<|im_end|>\\n\"\n",
    "\n",
    "    if not prompt.startswith(\"<|im_start|>system\\n\"):\n",
    "        prompt = \"<|im_start|>system\\n<|im_end|>\\n\" + prompt\n",
    "\n",
    "    prompt = prompt + \"<|im_start|>assistant\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-7b\",\n",
    "    tokenizer_name=\"/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-7b\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 10, \"top_p\": 0.75},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs=dict(trust_remote_code=True,torch_dtype=torch.bfloat16,do_sample=True), # 只能这样设置，会传回原来的huggingface接口\n",
    "    tokenizer_kwargs=dict(trust_remote_code=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "食神项目目前主要目标如下：\n",
      "\n",
      "1. 建立一个基于大语言模型和检索增强技术的综合问答系统，能够处理语音、文字、图片等多种输入形式。\n",
      "2. 利用上海人工智能实验室开源模型internlm-chat-7b（包含1代和2代），在XiaChuFang Recipe Corpus提供的1,520,327种中国食谱数据集上进行LoRA微调，形成shishen2_full模型。\n",
      "3. 使用langchain将微调后的模型与chroma或faiss向量数据库整合，实现RAG检索增强的效果。\n",
      "4. 前端基于streamlit实现与用户的交互。\n",
      "5. 确保模型在较小的显存配置中能够顺利运行，同时保证生图内容准确、生图速度快，等待时间短。\n",
      "6. 使用Taiyi-Stable-Diffusion-1B-Chinese-v0.1模型作为文生图模型，生成菜谱更加接近中国人的理解水平，同时满足时延要求。\n",
      "\n",
      "7. 持续优化和改进系统性能，提高用户满意度。\n",
      "8. 推广和宣传项目，吸引更多用户使用。\n",
      "9. 不断更新和扩充数据集。\n",
      "\n",
      "10. \n",
      "🎓📚 书生浦语大模型训练营第二期主要涵盖以下几个方面：\n",
      "\n",
      "1. 轻松分钟玩转书生·浦语大模型趣味 Demo 🎮\n",
      "2. XTuner 微调 LLM:1.8B、多模态、Agent 🔬\n",
      "3. LMDeploy 量化部署 LLM 实践 🚀\n",
      "4. Lagent & AgentLego 智能体应用搭建 🤖\n",
      "5. OpenCompass 大模型评测实战 📊\n",
      "6. 大模型微调数据构造 📈\n",
      "7. 彩蛋：平台工具类补充课程 🎁\n",
      "\n",
      "这些内容将带领学员深入了解书生·浦语大模型的各个方面，从理论知识到实践操作，全方位提升学员的学习能力和项目实践经验。\n",
      "\n",
      "课前\n",
      "\n",
      "2.1.1 注册算力平台\n",
      "\n",
      "1.访问问卷地址：https://www.wjx.cn/vm/tUX8dEV.aspx?udsid=2146\n",
      "\n",
      "2.点击右下角“查询结果”，如下图所示；\n",
      "\n",
      "\n",
      "\n",
      "3.复制序号及核销码，\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "query_engine = index.as_query_engine()\n",
    "response_god = query_engine.query(\"食神项目目前主要目标是什么,用中文分点列出\")\n",
    "response_puyu=query_engine.query(\"书生浦语大模型训练营第二期主要有什么内容,分点概括，并在每条头部搭配适宜的emoji\")\n",
    "clear_output()\n",
    "print(response_god)\n",
    "print(response_puyu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
