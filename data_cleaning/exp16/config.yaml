# global parameters
project_name: 'recipe-dedup'  # 项目名称
dataset_path: '/root/cook-data/recipe_corpus_juicer.json'  # 数据加载路径
export_path: '/root/cook-data/recipe_corpus_dedup.json'  # 数据保存路径
np: 16  # number of subprocess to process your dataset

# 使用下述key进行相似度比较
text_keys: 'name'

# 开启tracer可以记录过滤掉的数据，可以通过事后查看防止高质量数据被过滤
open_tracer: true
trace_num: 1000

# 开启算子融合，加速
op_fusion: true


# process schedule
# a list of several process operators with their arguments
# 中文注释的是需要我们调整的重要参数
# 英文注释的参数都是默认
process:

  # Deduplicator ops
  - document_deduplicator: # deduplicate text samples using md5 hashing exact matching method
      lowercase: true                                           # whether to convert text to lower case
      ignore_non_character: true                                # whether to ignore non-alphabet characters, including whitespaces, digits, and punctuations

  # Filter ops
  - text_length_filter: # 文本长度上限与下限
      min_len: 2                                              # 最短文本长度
      max_len: 30                                             # 最长文本长度
  
  - alphanumeric_filter: # 限定中文、英文、数字等有效字符占原始文本的比例
      tokenization: false                                     # whether to count the ratio of alphanumeric to the total number of tokens.
      min_ratio: 0.7                                          # 最低占比

  - character_repetition_filter: # 限定重复character占原始文本的比例
      rep_len: 20                                             # repetition length for char-level n-gram
      max_ratio: 0.1                                          # 最高占比

  - language_id_score_filter: # 限定原始文本的最低语言分数
      lang: zh                                                # keep text in what language
      min_score: 0.14                                          # 最低语言分数

  - perplexity_filter: # 限定原始文本的最大困惑度
      lang: zh                                                # compute perplexity in what language
      max_ppl: 17306                                           # 最大困惑度

  # Deduplicator ops
  - document_simhash_deduplicator: # 采用SimHash算法进行去重
      tokenization: 'character'                               # 文本分割方法  One of [space, punctuation, character]
      window_size: 6                                          # 同上
      num_blocks: 6                                           # 需要满足num_blocks > hamming_distance
      hamming_distance: 4                                     # 当两个数据 hamming 距离 <= 4时，舍弃其中一个
      lowercase: true                                         # whether to convert text to lower case
      ignore_pattern: null                                    # whether to ignore sub-strings with specific pattern when computing simhash.

  - document_minhash_deduplicator: # 采用MinHash算法进行去重
      tokenization: 'sentencepiece'                           # 文本分割方法 One of [space, punctuation, character, sentencepiece]
      window_size: 6                                          # 6-gram窗口（每个窗口包含6个字符）
      num_permutations: 256                                   # number of permutations in minhash computing
      jaccard_threshold: 0.7                                  # 当两个数据 jaccard 相似度（类似交并比）>= 0.7时，舍弃其中一个
      num_bands: null                                         # number of bands in LSH. Default it's None, and it will be determined by an optimal params computation algorithm by minimize the weighted sum of probs of False Positives and False Negatives
      num_rows_per_band: null                                 # number of rows in each band in LSH. Default it's None, and it will be determined by an optimal params computation algorithm
      lowercase: true                                         # whether to convert text to lower case
      ignore_pattern: null                                    # whether to ignore sub-strings with specific pattern when computing simhash.
      tokenizer_model: '/root/models/Shanghai_AI_Laboratory/internlm2-chat-1_8b/tokenizer.model'                                  # path for the sentencepiece model, used for sentencepiece tokenization.

